# Mini-Project 11: Data Pipeline with databricks
#### The purpose of this project is to create a data pipeline through databricks, utilizing a data source and a data sink, and generate a video or notebook report on _______

#### We are making a model (can use scikit learn or databricks)

#### Requirements:

- [X] Create a data pipeline using Databricks
- [X] Include at least one data source and one data sink
- [X] Pipeline functionality 
- [X] CI/CD pipeline
- [X] Data source and sink configuration 
- [X] README.md
- [x] Databricks notebook or script
- [x] Document or video demonstrating the pipeline


---
### Folder Navigation
##### Here is a quick overview of how the folders are structured for this project:
---
- Project Folder
    - .devcontainer
        - devcontainer.json
        - Dockerfile
    - .github
        - workflows
            - main.yml
    - data
        - Table 1 csv file
        - Table 2 csv file
    - SQL_files
        - complex_query.py
        - extract.py
        - transform.py
    - ETL_complex_diagram (arch diagram screenshot)
    - main.py
    - make_test_results (screenshot of local tests)
    - Makefile
    - query_image.png (screenshot of .md log results)
    - query_log.md (markdown that logs all queries made)
    - README.md
    - requirements.txt
    - test_main.py
---
### Workflow Summary and Explanation
##### This project contains the following dependencies:
- pylint == 2.15.3
- black == 22.3.0
- pytest == 7.1.3
- ruff == 0.0.284
- fire == 0.7.0
- requests == 2.32.3
- pandas == 2.2.2
- python-dotenv == 1.0.1
- databricks-sql-connector == 3.4.0
---




